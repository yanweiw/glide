<!DOCTYPE html>
<html lang="en">

<head>
    <style>
        .center {
        width: 100%;
        }

        .video-container {
            position: relative;
            width: 100%;
            height: 0;
            padding-bottom: 56.25%; /* 16:9 aspect ratio */
            overflow: hidden;
        }

        .full-width-video {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
        }
        .caption {
            text-align: center;
            margin-top: auto; /* Pushes the caption to the bottom of the container */
        }        
        .transparent-image {
            opacity: 0.7; /* Set opacity to 50% */
        }
    </style>
</head>

<head>
    <!-- Title -->
    <title>Grounding Language Plans in Demonstrations</title>

    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="Grounding Language Plans in Demonstrations through Counterfactual Perturbations">
    <meta name="keywords" content="Grounding LLM, Learning Mode Abstractions for Manipulation, Learning from Demonstration, Robotics, Task and Motion Planning">

    <!-- Bootstrap core CSS -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
    <!-- https://fontawesome.com/cheatsheet -->
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-79592980-2"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }
        gtag('js', new Date());

        gtag('config', 'UA-79592980-2');
    </script>

</head>


<body>
    <!-- <nav class="navbar navbar-expand-md navbar-dark fixed-top bg-dark"> -->
    <nav class="navbar navbar-expand-md fixed-top navbar-dark" style="background-color: #A31F34;">
        <a class="navbar-brand" href="#">Learning Grounding Classifiers for LLM Planning </a>

        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarToggle">
            <span class="navbar-toggler-icon"></span>
        </button>

        <div class="collapse navbar-collapse" id="navbarToggle">
            <ul class="navbar-nav ml-auto">
                <li class="nav-item">
                    <a class="nav-link" href="#">Home</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="#Abstract">Abstract</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="#Paper">Paper</a>
                </li>              
                <li class="nav-item">
                    <a class="nav-link" href="#Talk">Talk</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="#Method">Method</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="#Experiments">Experiments</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="#Related">Related Works</a>
                </li>                  
            </ul>
        </div>
    </nav>
    <br>
    <div class="container" style="padding-top: 80px; font-size: 20px">
        <div align="center">
            <h2 class="text-center" align="center">
                <strong>GLiDE</strong>: <strong><u>G</u></strong>rounding <strong><u>L</u></strong></strong>anguage Plans <strong><u>i</u></strong>n <strong><u>De</u></strong>monstrations through Counterfactual Perturbations
            </h2><br>
            <h6>
                <a href="https://yanweiw.github.io/" target="_blank">Yanwei Wang</a>&nbsp;&nbsp;&nbsp;&nbsp;
                <a href="https://zswang666.github.io/" target="_blank">Tsun-Hsuan Wang</a>&nbsp;&nbsp;&nbsp;&nbsp;                
                <a href="https://jiayuanm.com" target="_blank">Jiayuan Mao</a>&nbsp;&nbsp;&nbsp;&nbsp;
                <a href="https://www.hageneaux.com/" target="_blank">Michael Hagenow</a>&nbsp;&nbsp;&nbsp;&nbsp;
                <a href="https://interactive.mit.edu/about/people/julie" target="_blank">Julie Shah</a><br>
            </h6>
            <small>Interactive Robotics Lab / CSAIL</small> <br>
            <small>Massachusetts Institute of Technology</small>
        </div>
    </div><br>



    <div class="container">
        <div align="center">
        <video autoplay loop muted width="80%" height="auto">
            <source src="figs/ICLR_2024_intro_small.mp4" type="video/mp4">
        </video>
        </div>
    </div><br><br>


    <!-- Abstract -->
    <div class="container">
        <h4 id="Abstract" style="padding-top: 70px; margin-top: -80px; ">Abstract</h4>
        <hr>

        <div class="container" style="padding-top: 10px; font-size: 20px">
            <div align="center">
                <div class="center">
                    <img class="img-responsive img-rounded" src="figs/framework.png" style="width:70%" alt="">
                </div>
            </div>
        </div><br>

        <div style="text-align: justify">
        Grounding the common-sense reasoning of Large Language Models (LLMs) in physical domains remains a pivotal yet unsolved problem for embodied AI. Whereas prior works have focused on leveraging LLMs directly for planning in symbolic spaces, this work uses LLMs to guide the search of task structures and constraints implicit in multi-step demonstrations. Specifically, we borrow from manipulation planning literature the concept of mode families, which group robot configurations by specific motion constraints, to serve as an abstraction layer between the high-level language representations of an LLM and the low-level physical trajectories of a robot. By replaying a few human demonstrations with synthetic perturbations, we generate coverage over the demonstrations' state space with additional successful executions as well as counterfactuals that fail the task. Our explanation-based learning framework trains an end-to-end differentiable neural network to predict successful trajectories from failures and as a by-product learns classifiers that ground low-level states and images in mode families without dense labeling. The learned grounding classifiers can further be used to translate language plans into reactive policies in the physical domain in an interpretable manner. We show our approach improves the interpretability and reactivity of imitation learning through 2D navigation and simulated and real robot manipulation tasks.
        </div>
    </div><br><br>

    <!-- Paper -->
    <div class="container">
        <h4 id="Paper" style="padding-top: 70px; margin-top: -80px;">Paper</h4>
        <hr>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2403.17124">
                    <papertitle>Grounding Language Plans in Demonstrations through Counterfactual Perturbations</papertitle>
                </a><br>
                <strong>Yanwei Wang</strong>,
                Tsun-Hsuan Wang,
                Jiayuan Mao,
                Michael Hagenow,
                Julie Shah
              <em><br>
              <a href="https://arxiv.org/abs/2403.17124">arxiv</a> /
              <a href="https://openreview.net/forum?id=qoHeuRAcSl">review</a> /
              <a href="">code (coming soon) </a> /
              <a href="https://news.mit.edu/2024/engineering-household-robots-have-little-common-sense-0325">MIT News</a> /
              <a href="https://techcrunch.com/2024/03/25/large-language-models-can-help-home-robots-recover-from-errors-without-human-help/">techcrunch</a>
              <br>
              <!-- <a href="" data-toggle="modal" data-target="#bibtex">bibtex</a><br> -->
              <strong>ICLR 2024</strong> (<strong style="color:red;">Spotlight</strong>, acceptance rate: 5%)<br>
              </em><br>
            </td>

    </div><br><br>


    <div class="container">
        <h4 id="Talk" style="padding-top: 30px; margin-top: -40px;">Talk</h4>
        <hr>
        
        <td style="padding:20px;width:25%;vertical-align:middle">

            <div class="container">
                <div align="center">
                    <video src="figs/iclr_talk.mp4" type="video/mp4" controls="controls" class="center">
                    </video>
                </div>
            </div><br>
        </td>
    </div><br><br>

    <div class="container">
        <h4 id="Method" style="padding-top: 30px; margin-top: -40px;">Method</h4>
        <hr>
    
        <td style="padding:20px;width:25%;vertical-align:middle">
    
            <div align="center">
                <div class="center">
                    <img src="figs/method.png" style="width:60%" alt="">
                </div>
            </div><br>
    
            <div style="text-align: justify">
                By prompting LLM to describe successful executions in terms of valid mode transitions via a feasibility
                matrix, and augmenting demonstrations with counterfactual perturbations, we train a fully differentiable
                pipeline to predict task execution outcomes. Since the fully-differentiable pipeline calculates overall
                trajectory success based on mode classification results of individual states, we can learn classifiers that
                achieve grounding as a by-product of predicting the overall task execution.
            </div>
        </td>
    </div><br><br>



    <div class="container">
        <h4 id="Experiments" style="padding-top: 30px; margin-top: -40px;">Experiments</h4>
        <hr>

            <div style='text-align: justify; width: 90%; font-size: 15pt; color:#A31F34'>
                Learned Mode Partitions of Configuration Space for 2D Navigation Tasks
                <p></p>
            </div>
            <div>
                The navigation domain serves as a 2D abstraction of the modal structure for multi-step manipulation tasks, where
                each polygon represents a different mode with its boundary showing the constraint. The goal is to traverse through
                the sequence consecutively as demonstrated until reaching the last colored polygon (e.g. mode 1 -> 2 -> 3 -> 4 -> 5). Mode transitions violating the demonstrated temporal ordering will lead to task failure (e.g. mode 1 -> 3/4/5).
                <p></p>
            </div>

            <div class="row">
                    <table class="center">
                        <tr>
                            <td style="width: 20%; text-align: center;">
                                <img src='figs/polygon_3a.png' width="90%">
                                <p></p>
                            </td>
                            <td style="width: 20%; text-align: center;">
                                <img src='figs/polygon_3b_new.png' width="90%">
                                <p></p>
                            </td>
                            <td style="width: 20%; text-align: center;">
                                <img src='figs/polygon_3c.png' width="90%">
                                <p></p>
                            </td>
                            <td style="width: 20%; text-align: center;">
                                <img src='figs/polygon_3d.png' width="90%">
                                <p></p>
                            </td>
                            <td style="width: 20%; text-align: center;">
                                <img src='figs/polygon_3e.png' width="90%" class="transparent-image">
                                <p></p>
                            </td>
                        </tr>
            
                        <tr> 
                            <td style="width: 20%; text-align: center;">
                                <img src='figs/polygon_4a.png' width="90%">
                                <p></p>
                            </td>
                            <td style="width: 20%; text-align: center;">
                                <img src='figs/polygon_4b_new.png' width="90%">
                                <p></p>
                            </td>
                            <td style="width: 20%; text-align: center;">
                                <img src='figs/polygon_4c.png' width="90%">
                                <p></p>
                            </td>
                            <td style="width: 20%; text-align: center;">
                                <img src='figs/polygon_4d.png' width="90%">
                                <p></p>
                            </td>
                            <td style="width: 20%; text-align: center;">
                                <img src='figs/polygon_4e.png' width="90%" class="transparent-image">
                                <p></p>
                            </td>
                        </tr>
                        
                        <tr>
                            <td style="width: 20%; text-align: center;">
                                <img src='figs/polygon_5a.png' width="90%">
                                <p></p>
                            </td>
                            <td style="width: 20%; text-align: center;">
                                <img src='figs/polygon_5b_new.png' width="90%">
                                <p></p>
                            </td>
                            <td style="width: 20%; text-align: center;">
                                <img src='figs/polygon_5c.png' width="90%">
                                <p></p>
                            </td>
                            <td style="width: 20%; text-align: center;">
                                <img src='figs/polygon_5d.png' width="90%">
                                <p></p>
                            </td>
                            <td style="width: 20%; text-align: center;">
                                <img src='figs/polygon_5e.png' width="90%" class="transparent-image">
                                <p></p>
                            </td>
                        </tr>

                        <tr>
                            <td style="width: 20%; text-align: center;">
                                <p style="width: 90%; margin: 0 auto;">(a) Demonstrations and ground truth modes</p>
                            </td>
                            <td style="width: 20%; text-align: center;">
                                <p style="width: 90%; margin: 0 auto;">(b) Grounding learned by GLiDE</p>
                            </td>
                            <td style="width: 20%; text-align: center;">
                                <p style="width: 90%; margin: 0 auto;">(c) GLiDE without counterfactual data</p>
                            </td>
                            <td style="width: 20%; text-align: center;">
                                <p style="width: 90%; margin: 0 auto;">(d) GLiDE with incorrect number of modes</p>
                            </td>
                            <td style="width: 20%; text-align: center;">
                                <p style="width: 90%; margin: 0 auto;">(e) Baseline: similarity-based clustering</p>
                            </td>
                        </tr>
                    </table>
            </div><br><br>


            <div style='text-align: justify; width: 90%; font-size: 15pt; color:#A31F34'>
                Learned Mode Partitions Explain Why Some Perturbed Trajectories Succeed While Others Fail
                <p></p>
            </div>
            <div>
                Learned classifier explains success trajectories by checking if all transitions incur 0 cost (shown by the whiteness of dots). It also explains failure trajectories by checking if there exists at least one invalid transition (indicated by the blackness of dots).
                <p></p>
            </div>
            <div class="row">
                    <table class="center">            
                        <tr>
                            <td style="width: 25%; text-align: center;">
                                <img src='figs/polygon_succ_1.png' width="80%">
                                <p></p>
                            </td>
                            <td style="width: 25%; text-align: center;">
                                <img src='figs/polygon_succ_2.png' width="80%">
                                <p></p>
                            </td>

                            <td style="width: 25%; text-align: center;">
                                <img src='figs/polygon_fail_1.png' width="80%">
                                <p></p>
                            </td>
                            <td style="width: 25%; text-align: center;">
                                <img src='figs/polygon_fail_2.png' width="80%">
                                <p></p>
                            </td>
                        </tr>
                        
                        <tr>
                            <td style="width: 25%; text-align: center;">
                                <p style="width: 80%; margin: 0 auto;">Successful Execution 1</p>
                            </td>
                            <td style="width: 25%; text-align: center;">
                                <p style="width: 80%; margin: 0 auto;">Successful Execution 2</p>
                            </td>

                            <td style="width: 25%; text-align: center;">
                                <p style="width: 80%; margin: 0 auto;">Failing Counterfactual 1</p>
                            </td>
                            <td style="width: 25%; text-align: center;">
                                <p style="width: 80%; margin: 0 auto;">Failing Counterfactual 2</p>
                            </td>
                        </tr>
                    </table>
            </div><br><br>
             
            <div style='text-align: justify; width: 90%; font-size: 15pt; color:#A31F34'>
                Learned Mode Partitions Allow Planning Reactive Behavior for 2D Navigation Tasks
                <p></p>
            </div>
            <div>
                The learned grounding classifier can be used to segment whole demonstrations into sub-sequences for each mode, for which we extract the sub-goal (the average crossing point at each boundary) for each mode. Additionally, the explicit mode partitions allow us construct a potential flow controller for each mode that directs the system to reach each sub-goal without leaving the current mode prematurely. Should the execution be derailed by external perturbations, planning without and with the learned mode boundaries can lead to execution failures (left) or successes (right), respectively.
                <p></p>
            </div>
            <div class="row">
                <table class="center">
                    <tr>
                        <td style="width: 50%; text-align: center;">
                            <div style="width: 70%; margin: 0 auto;">
                                <video src="figs/polygon_no_replanning.mp4" type="video/mp4" controls="controls" class="center">
                                </video>
                            </div>
                        </td>
                        <td style="width: 50%; text-align: center;">
                            <div style="width: 70%; margin: 0 auto;">
                                <video src="figs/polygon_replanning.mp4" type="video/mp4" controls="controls" class="center">
                                </video>
                            </div>
                        </td>
                    </tr>

                    <tr>
                        <td style="width: 50%; text-align: center;">
                            <p style="width: 70%; margin: 0 auto;">Planning without learned grounding leads to failures due to invalid mode transitions</p>
                        </td>

                        <td style="width: 50%; text-align: center;">
                            <p style="width: 70%; margin: 0 auto;">Planning with learned grounding leads to successful recovery that obeys mode constraints</p>
                        </td>
                    </tr>
                </table>
            </div><br><br>

            <div style='text-align: justify; font-size: 15pt; color:#A31F34'>
                Self-Supervised Data Collection for 2D Navigation Tasks
                <p></p>
            </div>
            <div>
                To generate the data for training the grounding classifier, we first collect a few successful demonstrations from humans. We then apply synthetic perturbations to demonstration replays to generate additional successful trajectories and failing counterfactuals. Assuming an automatic reset mechanism and a labeling function that decides the trajectory execution outcome, we can collect a large dataset without humans in the loop.
                <p></p>
            </div>
            <div class="row">
                <table class="center">
                    <tr>
                        <td style="width: 16%; text-align: center;">
                            <img src='figs/tracing_setup.png' width="95%">
                            <p></p>
                        </td>
                        <td style="width: 42%; text-align: center;">
                            <div style="width: 95%; margin: 0 auto;">
                                <video src="figs/tracing_teaching_2x.mp4" type="video/mp4" controls="controls" class="center">
                                </video>
                            </div>
                        </td>
                        <td style="width: 42%; text-align: center;">
                            <div style="width: 95%; margin: 0 auto;">
                                <video src="figs/tracing_replay_2:4x.mp4" type="video/mp4" controls="controls" class="center">
                                </video>
                            </div>
                        </td>
                    </tr>
            
                    <tr>
                        <td style="width: 16%; text-align: center;">
                            <p style="width: 90%; margin: 0 auto;">Data Collection Setup</p>
                        </td>
                        <td style="width: 42%; text-align: center;">
                            <p style="width: 70%; margin: 0 auto;">Human demonstrations: Mode 1 -> 2 -> 3 -> 4 -> 5. Invalid mode transitions: 1 -> 3 or 1 -> 4 or 1 -> 5 </p>
                        </td>
                        <td style="width: 42%; text-align: center;">
                            <p style="width: 70%; margin: 0 auto;">Add synthetic perturbations to demonstration replays with auto-labeling and auto-reset</p>
                        </td>
                    </tr>
                </table>
            </div><br><br>

            <div style='text-align: justify; font-size: 15pt; color:#A31F34'>
                Learning Image-based Grounding Classifier 2D Navigation Tasks
                <p></p>
            </div>
            <div>
                Given a dataset of perturbed replays with only sparse labels, we learn a classifier that maps image observations to the their corresponding modes shown by distinct colors. 
                <p></p>
            </div>
            <div class="row">
                <table class="center">
                    <tr>
                        <td style="width: 100%; text-align: center;">
                            <img src='figs/tracing_img_classifier.png' width="80%">
                            <p></p>
                        </td>
                    </tr>
            
                    <!-- <tr>
                        <td style="width: 100%; text-align: center;">
                            <p style="width: 80%; margin: 0 auto;"></p>
                        </td>
                    </tr> -->
                </table>
            </div><br><br>


            <div style='text-align: justify; font-size: 15pt; color:#A31F34'>
                Learned Grounding Classifier Enables Replanning for 2D Navigation Tasks
                <p></p>
            </div>
            <div>
                Once we learn the classifier, we can use it to generate a mode-informed policy that replans when a perturbation would lead the original policy to incur an invalid mode transition (e.g., white directly to blue/yellow/red)
                <p></p>
            </div>
            <div class="row">
                <table class="center">
                    <tr>
                        <td style="width: 50%; text-align: center;">
                            <div style="width: 90%; margin: 0 auto;">
                                <video src="figs/tracing_perturb_no_recovery.mp4" type="video/mp4" controls="controls" class="center">
                                </video>
                            </div>
                        </td>
                        <td style="width: 50%; text-align: center;">
                            <div style="width: 90%; margin: 0 auto;">
                                <video src="figs/tracing_perturb_recovery.mp4" type="video/mp4" controls="controls" class="center">
                                </video>
                            </div>
                        </td>
                    </tr>
            
                    <tr>
                        <td style="width: 50%; text-align: center;">
                            <p style="width: 90%; margin: 0 auto;">Naive policy: without knowledge of the modes, the robot simply continues after perturbation which can lead to invalid mode transitions. </p>
                        </td>
                        <td style="width: 50%; text-align: center;">
                            <p style="width: 90%; margin: 0 auto;">Mode-conditioned policy: By accurately recognizing the underlying modes, our method allows for recovery when perturbations cause unexpected mode transitions.</p>
                        </td>
                    </tr>
                </table>
            </div><br><br>


            <div class="row">
                <div class="col-md-12">
                    <table class="center">
                        <tr style=font-size:15pt>
                            <td style='text-align:left; vertical-align:middle; color:#A31F34' colspan="2">
                                Learned Mode Segmentation of Demonstrations for Robosuite Manipulation Tasks
                                <p>
                            </td>
                        </tr>
            
                        <tr>
                            <td width="50%">
                                <img src='figs/robosuite_segmentation.png' width="100%">
                            </td>
            
                            <td align="right">
                                <div style="text-align: justify; width: 90%;">
                                    <strong>(Left)</strong> We manually set a ground truth based on RoboSuite state information to
                                    benchmark our learned mode segmentation, visualized in the left figure. <span
                                        style="color:#A31F34">Can Task: </span> pick up the can and place it in the bottom-right
                                    bin. <span style="color:#A31F34">Lifting Task: </span>pick up the block above a specified
                                    height. <span style="color:#A31F34">Square Peg Task: </span>pick up the square nut and place it
                                    on the square peg. You can view pictures of all of the classification results <a
                                        href="https://drive.google.com/drive/folders/1qapomOjcBTvmdEXVCpfz9o6VbAckfSzx">here</a>.
                                    <strong>(Below)</strong> To show the learned grounding can facilitate planning using LLMs, we
                                    apply perturbations to Behavioral Cloning (BC) rollouts below. BC performance drops off
                                    significantly with perturbations, but our method generally recovers better after the robot drops
                                    the can as the robot will be directed to pick up the can again.
                                </div>
                            </td>
                        </tr>
            
                    </table>
                </div>
            </div><br><br>
        
                
            <div class="row">
                <div class="col-md-12">
                    <!-- <ul> -->
                    <table class="center">
                        <tr style=font-size:15pt>
                            <td style='text-align:left; vertical-align:middle; color:#A31F34' colspan="3">
                                Learned Grounding facilitates replanning with LLMs to improve execution success rate for RoboSuite Tasks
                                <p>
                            </td>
                        </tr>
            
                        <tr>
                            <td style="width: 30%;">
                                <div class="video-container">
                                    <video controls class="full-width-video">
                                        <source src="figs/can_BC.mp4" type="video/mp4">
                                    </video>
                                </div>
                                <!-- <p style="text-align: center;">BC rollouts (93% success)</p> -->
                            </td>
            
                            <td style="width: 30%;">
                                <div class="video-container">
                                    <video controls class="full-width-video">
                                        <source src="figs/can_BC_pert.mp4" type="video/mp4">
                                    </video>
                                </div>
                                <!-- <p style="text-align: center;">BC rollouts + perturbations (20% success)</p> -->
                            </td>
            
                            <td style="width: 30%;">
                                <div class="video-container">
                                    <video controls class="full-width-video">
                                        <source src="figs/can_MC_pert.mp4" type="video/mp4">
                                    </video>
                                </div>
                                <!-- <p style="text-align: center;">Mode-conditioned policy + perturbations (40% success)</p> -->
                            </td>
                        </tr>
            
                        <tr>
                            <td>
                                <div class="caption">
                                    Behavior cloning (BC) rollouts: 93% success
                                </div>
                            </td>
                            <td>
                                <div class="caption">
                                    BC rollouts (perturbed): 20% success
                                </div>
                            </td>
                            <td>
                                <div class="caption">
                                    Mode-conditioned BC (perturbed): 40% success
                                </div>
                            </td>
                        </tr>
                    </table>
                </div>
            </div><br><br>

            <div style='text-align: justify; font-size: 15pt; color:#A31F34'>
                Learned Grounding Classifier Enables Replanning for Real-World Scooping Task
                <p></p>
            </div>
            <div>
                A scooping task that requires the robot to transport at least a marble from one bowl to the other. Given external perturbations, the robot may drop marbles during transport, leading to a failed execution. When an imitation policy does not pay attention to the classified modes (shown on the right in the video), it cannot recover from such failures. In constrast, a mode-conditioned policy can replan and recover from such perturbations. 
                <p></p>
            </div>
            <div class="row">
                <table class="center">
                    <tr>
                        <td style="width: 50%; text-align: center;">
                            <div style="width: 95%; margin: 0 auto;">
                                <video src="figs/scoop_non_reactive_2x.mp4" type="video/mp4" controls="controls"
                                    class="center">
                                </video>
                            </div>
                        </td>
                        <td style="width: 50%; text-align: center;">
                            <div style="width: 95%; margin: 0 auto;">
                                <video src="figs/scoop_reactive_2x.mp4" type="video/mp4" controls="controls" class="center">
                                </video>
                            </div>
                        </td>
                    </tr>
            
                    <tr>
                        <td style="width: 50%; text-align: center;">
                            <p style="width: 90%; margin: 0 auto;">Mode-agnostic BC cannot recover from task-level perturbations (e.g. dropping all marbles) </p>
                        </td>
                        <td style="width: 50%; text-align: center;">
                            <p style="width: 90%; margin: 0 auto;">Mode-conditioned BC enabled by the learned grounding can leverage LLM to replan</p>
                        </td>
                    </tr>
                </table>
            </div><br><br>
    
                
                
                
                

    </div><br>


    <!-- Poster -->
    <div class="container">
        <h4 id="Related" style="padding-top: 30px; margin-top: -40px;">Related Works - Prior work that GLiDE extends by learning instead of engineering sensor models</h4>
        <hr>


        <table class="center">
        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                    <img src='figs/robot_1.jpg' width="100%">
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://yanweiw.github.io/tli/">
                    <papertitle>Temporal Logic Imitation: Learning Plan-Satisficing Motion Policies from Demonstrations
                    </papertitle>
                </a>
                <br>
                <strong>Yanwei Wang</strong>,
                Nadia Figueroa, Shen Li, Ankit Shah, Julie Shah
                <em><br>
                    <a href="https://arxiv.org/abs/2206.04632">arxiv</a>
                    /
                    <a href="https://github.com/yanweiw/tli">code</a>
                    /
                    <a href="https://yanweiw.github.io/tli/">project page</a><br>
                    <strong>CoRL 2022</strong> (<strong style="color:red;">Oral</strong>, acceptance rate: 6.5%) <br>
                    <strong>IROS 2023 Workshop</strong> (<strong style="color:red;"> Best Student Paper</strong>, Learning Meets
                    Model-based Methods for Manipulation and Grasping Workshop)
                </em><br>
                <p>We present a continuous motion imitation method that can provably satisfy any discrete plan specified by a
                    Linear Temporal Logic (LTL) formula. Consequently, the imitator is robust to both task- and motion-level
                    disturbances and guaranteed to achieve task success.</p>
            </td>
        </table>


    </div><br><br>

    <!-- Bootstrap core JavaScript -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.11.0/umd/popper.min.js" integrity="sha384-b/U6ypiBEHpOf/4+1nzFpr53nxSS+GLCkfwBdFNTxtclqqenISfwAzpKaMNFNmj4" crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/js/bootstrap.min.js" integrity="sha384-h0AbiXch4ZDo7tp9hKZ4TsHbi047NrKGLO3SEJAg45jXxnGIfYzk4Si90RDIqNm1" crossorigin="anonymous"></script>

</body>

</html>