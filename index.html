<!DOCTYPE html>
<html lang="en">

<head>
    <style>
        .center {
        width: 100%;
        }

        .video-container {
            position: relative;
            width: 100%;
            height: 0;
            padding-bottom: 56.25%; /* 16:9 aspect ratio */
            overflow: hidden;
        }

        .full-width-video {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
        }
        .caption {
            text-align: center;
            margin-top: auto; /* Pushes the caption to the bottom of the container */
        }        
        .transparent-image {
            opacity: 0.7; /* Set opacity to 50% */
        }
    </style>
</head>

<head>
    <!-- Title -->
    <title>Grounding Language Plans in Demonstrations</title>

    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="Grounding Language Plans in Demonstrations through Counterfactual Perturbations">
    <meta name="keywords" content="Grounding LLM, Learning Mode Abstractions for Manipulation, Learning from Demonstration, Robotics, Task and Motion Planning">

    <!-- Bootstrap core CSS -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
    <!-- https://fontawesome.com/cheatsheet -->
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-79592980-2"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }
        gtag('js', new Date());

        gtag('config', 'UA-79592980-2');
    </script>

</head>


<body>
    <!-- <nav class="navbar navbar-expand-md navbar-dark fixed-top bg-dark"> -->
    <nav class="navbar navbar-expand-md fixed-top navbar-dark" style="background-color: #A31F34;">
        <a class="navbar-brand" href="#">Learning Grounding Classifiers for LLM Planning </a>

        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarToggle">
            <span class="navbar-toggler-icon"></span>
        </button>

        <div class="collapse navbar-collapse" id="navbarToggle">
            <ul class="navbar-nav ml-auto">
                <li class="nav-item">
                    <a class="nav-link" href="#">Home</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="#Abstract">Abstract</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="#Paper">Paper</a>
                </li>              
                <li class="nav-item">
                    <a class="nav-link" href="#Talk">Method</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="#MoreVideos">Experiments</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="#Related">Related Works</a>
                </li>                  
            </ul>
        </div>
    </nav>
    <br>
    <div class="container" style="padding-top: 80px; font-size: 20px">
        <div align="center">
            <h2 class="text-center" align="center">
                <strong>GLiDE</strong>: <strong><u>G</u></strong>rounding <strong><u>L</u></strong></strong>anguage Plans <strong><u>i</u></strong>n <strong><u>De</u></strong>monstrations through Counterfactual Perturbations
            </h2><br>
            <h6>
                <a href="https://yanweiw.github.io/" target="_blank">Yanwei Wang</a>&nbsp;&nbsp;&nbsp;&nbsp;
                <a href="https://zswang666.github.io/" target="_blank">Tsun-Hsuan Wang</a>&nbsp;&nbsp;&nbsp;&nbsp;                
                <a href="https://jiayuanm.com" target="_blank">Jiayuan Mao</a>&nbsp;&nbsp;&nbsp;&nbsp;
                <a href="https://www.hageneaux.com/" target="_blank">Michael Hagenow</a>&nbsp;&nbsp;&nbsp;&nbsp;
                <a href="https://interactive.mit.edu/about/people/julie" target="_blank">Julie Shah</a><br>
            </h6>
            <small>Interactive Robotics Lab / CSAIL</small> <br>
            <small>Massachusetts Institute of Technology</small>
        </div>
    </div><br>



    <div class="container">
        <div align="center">
        <video autoplay loop muted width="80%" height="auto">
            <source src="figs/ICLR_2024_intro_small.mp4" type="video/mp4">
        </video>
        </div>
    </div><br><br>


    <!-- Abstract -->
    <div class="container">
        <h4 id="Abstract" style="padding-top: 70px; margin-top: -80px; ">Abstract</h4>
        <hr>

        <div class="container" style="padding-top: 10px; font-size: 20px">
            <div align="center">
                <div class="center">
                    <img class="img-responsive img-rounded" src="figs/framework.png" style="width:70%" alt="">
                </div>
            </div>
        </div><br>

        <div style="text-align: justify">
        Grounding the common-sense reasoning of Large Language Models (LLMs) in physical domains remains a pivotal yet unsolved problem for embodied AI. Whereas prior works have focused on leveraging LLMs directly for planning in symbolic spaces, this work uses LLMs to guide the search of task structures and constraints implicit in multi-step demonstrations. Specifically, we borrow from manipulation planning literature the concept of mode families, which group robot configurations by specific motion constraints, to serve as an abstraction layer between the high-level language representations of an LLM and the low-level physical trajectories of a robot. By replaying a few human demonstrations with synthetic perturbations, we generate coverage over the demonstrations' state space with additional successful executions as well as counterfactuals that fail the task. Our explanation-based learning framework trains an end-to-end differentiable neural network to predict successful trajectories from failures and as a by-product learns classifiers that ground low-level states and images in mode families without dense labeling. The learned grounding classifiers can further be used to translate language plans into reactive policies in the physical domain in an interpretable manner. We show our approach improves the interpretability and reactivity of imitation learning through 2D navigation and simulated and real robot manipulation tasks.
        </div>
    </div><br><br>

    <!-- Paper -->
    <div class="container">
        <h4 id="Paper" style="padding-top: 70px; margin-top: -80px;">Paper</h4>
        <hr>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2403.17124">
                    <papertitle>Grounding Language Plans in Demonstrations through Counterfactual Perturbations</papertitle>
                </a><br>
                <strong>Yanwei Wang</strong>,
                Tsun-Hsuan Wang,
                Jiayuan Mao,
                Michael Hagenow,
                Julie Shah
              <em><br>
              <a href="https://arxiv.org/abs/2403.17124">arxiv</a> /
              <a href="https://openreview.net/forum?id=qoHeuRAcSl">review</a> /
              <a href="">code (coming soon) </a> /
              <a href="https://news.mit.edu/2024/engineering-household-robots-have-little-common-sense-0325">MIT News</a> /
              <a href="https://techcrunch.com/2024/03/25/large-language-models-can-help-home-robots-recover-from-errors-without-human-help/">techcrunch</a>
              <br>
              <!-- <a href="" data-toggle="modal" data-target="#bibtex">bibtex</a><br> -->
              <strong>ICLR 2024</strong> (<strong style="color:red;">Spotlight</strong>, acceptance rate: 5%)<br>
              </em><br>
            </td>

    </div><br><br>


    <div class="container">
        <h4 id="Talk" style="padding-top: 30px; margin-top: -40px;">Method</h4>
        <hr>
        
        <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="row justify-content-md-center">
                <!-- <div style='text-align: center; width: 90%; font-size: 18px; color:#A31F34; width: 70%;' >
                    <b>How TLI (yellow box) is related to prior work (gray boxes)</b>
                </div><br><br> -->
                <img src='figs/method.png' style="width:60%" align="center" alt="">
            </div><br>
            <div style="text-align: justify">
                By prompting LLM to describe successful executions in terms of valid mode transitions via a feasibility matrix, and augmenting demonstrations with counterfactual perturbations, we train a fully differentiable pipeline to predict task execution outcomes. Since the fully-differentiable pipeline calculates overall trajectory success based on mode classification results of individual states, we can learn classifiers that achieve grounding as a by-product of predicting the overall task execution.
            </div>
        </td>
    </div><br><br>



    <div class="container">
        <h4 id="MoreVideos" style="padding-top: 30px; margin-top: -40px;">Experiments</h4>
        <hr>

            <div style='text-align: justify; width: 90%; font-size: 15pt; color:#A31F34'>
                Learned Mode Partitions of Configuration Space for 2D Navigation Tasks
                <p></p>
            </div>
            <div>
                The navigation domain serves as a 2D abstraction of the modal structure for multi-step manipulation tasks, where
                each polygon represents a different mode with its boundary showing the constraint. The goal is to traverse through
                the sequence consecutively as demonstrated until reaching the last colored polygon.
                <p></p>
            </div>

            <div class="row">
                    <table class="center">
                        <tr>
                            <td style="width: 20%; text-align: center;">
                                <img src='figs/polygon_3a.png' width="90%">
                                <p></p>
                            </td>
                            <td style="width: 20%; text-align: center;">
                                <img src='figs/polygon_3b.png' width="90%">
                                <p></p>
                            </td>
                            <td style="width: 20%; text-align: center;">
                                <img src='figs/polygon_3c.png' width="90%">
                                <p></p>
                            </td>
                            <td style="width: 20%; text-align: center;">
                                <img src='figs/polygon_3d.png' width="90%">
                                <p></p>
                            </td>
                            <td style="width: 20%; text-align: center;">
                                <img src='figs/polygon_3e.png' width="90%" class="transparent-image">
                                <p></p>
                            </td>
                        </tr>
            
                        <tr> 
                            <td style="width: 20%; text-align: center;">
                                <img src='figs/polygon_4a.png' width="90%">
                                <p></p>
                            </td>
                            <td style="width: 20%; text-align: center;">
                                <img src='figs/polygon_4b.png' width="90%">
                                <p></p>
                            </td>
                            <td style="width: 20%; text-align: center;">
                                <img src='figs/polygon_4c.png' width="90%">
                                <p></p>
                            </td>
                            <td style="width: 20%; text-align: center;">
                                <img src='figs/polygon_4d.png' width="90%">
                                <p></p>
                            </td>
                            <td style="width: 20%; text-align: center;">
                                <img src='figs/polygon_4e.png' width="90%" class="transparent-image">
                                <p></p>
                            </td>
                        </tr>
                        
                        <tr>
                            <td style="width: 20%; text-align: center;">
                                <img src='figs/polygon_5a.png' width="90%">
                                <p></p>
                            </td>
                            <td style="width: 20%; text-align: center;">
                                <img src='figs/polygon_5b.png' width="90%">
                                <p></p>
                            </td>
                            <td style="width: 20%; text-align: center;">
                                <img src='figs/polygon_5c.png' width="90%">
                                <p></p>
                            </td>
                            <td style="width: 20%; text-align: center;">
                                <img src='figs/polygon_5d.png' width="90%">
                                <p></p>
                            </td>
                            <td style="width: 20%; text-align: center;">
                                <img src='figs/polygon_5e.png' width="90%" class="transparent-image">
                                <p></p>
                            </td>
                        </tr>

                        <tr>
                            <td style="width: 20%; text-align: center;">
                                <p style="width: 90%; margin: 0 auto;">(a) Demonstrations and ground truth modes</p>
                            </td>
                            <td style="width: 20%; text-align: center;">
                                <p style="width: 90%; margin: 0 auto;">(b) Grounding learned by GLiDE</p>
                            </td>
                            <td style="width: 20%; text-align: center;">
                                <p style="width: 90%; margin: 0 auto;">(c) GLiDE without counterfactual data</p>
                            </td>
                            <td style="width: 20%; text-align: center;">
                                <p style="width: 90%; margin: 0 auto;">(d) GLiDE with incorrect number of modes</p>
                            </td>
                            <td style="width: 20%; text-align: center;">
                                <p style="width: 90%; margin: 0 auto;">(e) Baseline: similarity-based clustering</p>
                            </td>
                        </tr>
                    </table>
            </div><br><br>


            <div style='text-align: justify; width: 90%; font-size: 15pt; color:#A31F34'>
                Learned Mode Partitions Explain Why Some Perturbed Trajectories Succeed While Others Fail
                <p></p>
            </div>
            <div>
                Learned classifier explains success trajectories by checking if all transitions incur 0 cost (shown by the whiteness of dots). It also explains failure trajectories by checking if there exists at least one invalid transition (indicated by the blackness of dots).
                <p></p>
            </div>
            <div class="row">
                    <table class="center">            
                        <tr>
                            <td style="width: 25%; text-align: center;">
                                <img src='figs/polygon_succ_1.png' width="80%">
                                <p></p>
                            </td>
                            <td style="width: 25%; text-align: center;">
                                <img src='figs/polygon_succ_2.png' width="80%">
                                <p></p>
                            </td>

                            <td style="width: 25%; text-align: center;">
                                <img src='figs/polygon_fail_1.png' width="80%">
                                <p></p>
                            </td>
                            <td style="width: 25%; text-align: center;">
                                <img src='figs/polygon_fail_2.png' width="80%">
                                <p></p>
                            </td>
                        </tr>
                        
                        <tr>
                            <td style="width: 25%; text-align: center;">
                                <p style="width: 80%; margin: 0 auto;">Successful Execution 1</p>
                            </td>
                            <td style="width: 25%; text-align: center;">
                                <p style="width: 80%; margin: 0 auto;">Successful Execution 2</p>
                            </td>

                            <td style="width: 25%; text-align: center;">
                                <p style="width: 80%; margin: 0 auto;">Failing Counterfactual 1</p>
                            </td>
                            <td style="width: 25%; text-align: center;">
                                <p style="width: 80%; margin: 0 auto;">Failing Counterfactual 2</p>
                            </td>
                        </tr>
                    </table>
            </div><br><br>
             
            <div style='text-align: justify; width: 90%; font-size: 15pt; color:#A31F34'>
                Learned Mode Partitions Allow Planning Reactive Behavior for 2D Navigation Tasks
                <p></p>
            </div>
            <div>
                The learned grounding classifier can be used to segment whole demonstrations into sub-sequences for each mode, for which we extract the sub-goal (the average crossing point at each boundary) for each mode. Additionally, the explicit mode partitions allow us construct a potential flow controller for each mode that directs the system to reach each sub-goal without leaving the current mode prematurely. Should the execution be derailed by external perturbations, planning without and with the learned mode boundaries can lead to execution failures (left) or successes (right), respectively.
                <p></p>
            </div>
            <div class="row">
                <table class="center">
                    <tr>
                        <td style="width: 50%; text-align: center;">
                            <div style="width: 70%; margin: 0 auto;">
                                <video src="figs/polygon_no_replanning.mp4" type="video/mp4" controls="controls" class="center">
                                </video>
                                <!-- <p style="text-align: center;">Planning without grounding leads to execution failure</p> -->
                            </div>
                        </td>
                        <td style="width: 50%; text-align: center;">
                            <div style="width: 70%; margin: 0 auto;">
                                <video src="figs/polygon_replanning.mp4" type="video/mp4" controls="controls" class="center">
                                </video>
                            </div>
                            <!-- <p style="text-align: center;">Planning with grounding leads to reactive behavior that succeed in the task</p> -->
                        </td>
                    </tr>

                    <tr>
                        <td style="width: 50%; text-align: center;">
                            <p style="width: 70%; margin: 0 auto;">Planning without learned grounding leads to failures due to invalid mode transitions</p>
                        </td>

                        <td style="width: 50%; text-align: center;">
                            <p style="width: 70%; margin: 0 auto;">Planning with learned grounding leads to successful recovery that obeys mode constraints</p>
                        </td>
                    </tr>
                </table>
            </div><br><br>

            <div class="row">
                <div class="col-md-12">
                    <table class="center">
                        <tr style=font-size:15pt>
                            <td style='text-align:left; vertical-align:middle; color:#A31F34' colspan="2">
                                Learned Mode Segmentation of Demonstrations in Robosuite Manipulation Tasks
                                <p>
                            </td>
                        </tr>
            
                        <tr>
                            <td width="50%">
                                <img src='figs/robosuite_segmentation.png' width="100%">
                            </td>
            
                            <td align="right">
                                <div style="text-align: justify; width: 90%;">
                                    <strong>(Left)</strong> We manually set a ground truth based on RoboSuite state information to
                                    benchmark our learned mode segmentation, visualized in the left figure. <span
                                        style="color:#A31F34">Can Task: </span> pick up the can and place it in the bottom-right
                                    bin. <span style="color:#A31F34">Lifting Task: </span>pick up the block above a specified
                                    height. <span style="color:#A31F34">Square Peg Task: </span>pick up the square nut and place it
                                    on the square peg. You can view pictures of all of the classification results <a
                                        href="https://drive.google.com/drive/folders/1qapomOjcBTvmdEXVCpfz9o6VbAckfSzx">here</a>.
                                    <strong>(Below)</strong> To show the learned grounding can facilitate planning using LLMs, we
                                    apply perturbations to Behavioral Cloning (BC) rollouts below. BC performance drops off
                                    significantly with perturbations, but our method generally recovers better after the robot drops
                                    the can as the robot will be directed to pick up the can again.
                                </div>
                            </td>
                        </tr>
            
                    </table>
                </div>
            </div><br><br>
        
                
            <div class="row">
                <div class="col-md-12">
                    <!-- <ul> -->
                    <table class="center">
                        <tr style=font-size:15pt>
                            <td style='text-align:left; vertical-align:middle; color:#A31F34' colspan="3">
                                Learned Grounding facilitates replanning with LLMs to improve execution success rate
                                <p>
                            </td>
                        </tr>
            
                        <tr>
                            <td style="width: 30%;">
                                <div class="video-container">
                                    <video controls class="full-width-video">
                                        <source src="figs/can_BC.mp4" type="video/mp4">
                                    </video>
                                </div>
                                <!-- <p style="text-align: center;">BC rollouts (93% success)</p> -->
                            </td>
            
                            <td style="width: 30%;">
                                <div class="video-container">
                                    <video controls class="full-width-video">
                                        <source src="figs/can_BC_pert.mp4" type="video/mp4">
                                    </video>
                                </div>
                                <!-- <p style="text-align: center;">BC rollouts + perturbations (20% success)</p> -->
                            </td>
            
                            <td style="width: 30%;">
                                <div class="video-container">
                                    <video controls class="full-width-video">
                                        <source src="figs/can_MC_pert.mp4" type="video/mp4">
                                    </video>
                                </div>
                                <!-- <p style="text-align: center;">Mode-conditioned policy + perturbations (40% success)</p> -->
                            </td>
                        </tr>
            
                        <tr>
                            <td>
                                <div class="caption">
                                    Behavior cloning (BC) rollouts: 93% success
                                </div>
                            </td>
                            <td>
                                <div class="caption">
                                    BC rollouts (perturbed): 20% success
                                </div>
                            </td>
                            <td>
                                <div class="caption">
                                    Mode-conditioned BC (perturbed): 40% success
                                </div>
                            </td>
                        </tr>
                    </table>
                </div>
            </div>
                
                <br>
                Migrating videos from the <a href="https://sites.google.com/view/grounding-plans">old website</a>. Coming soon.
                
                
                
                </div><br><br>

        <!-- <div class="row">
            <div class="col-md-12">
                <ul>
                    <table class="center">
                        <tr style=font-size:15pt>
                            <td style='text-align:center; vertical-align:middle; color:#A31F34' colspan="4">
                                Generalization to New Tasks by Reusing Learned Skills
                                <p>
                            </td>
                        </tr>
                        <tr>
                            <td width="25%">
                                <video width="90%" class="center" controls>
                                    <source src="figs/get_chicken_first.mp4" type="video/mp4">
                                </video>
                            </td>        
                            <td width="25%">
                                <video width="90%" class="center" controls>
                                    <source src="figs/get_broccoli_first.mp4" type="video/mp4">
                                </video>
                            </td>
                            <td width="25%">
                                <video width="90%" class="center" controls>
                                    <source src="figs/getting_only_chicken.mp4" type="video/mp4">
                                </video>
                            </td>
                            <td width="25%">
                                <video width="90%" class="center" controls>
                                    <source src="figs/continuously_getting_chicken.mp4" type="video/mp4">
                                </video>
                            </td>
                        </tr>
                    </table>
                    <div style="text-align: justify; width: 98%;">
                        LTL-DS generalizes to new task structures (encoded by LTL) by flexibly combining individual skills learned in
                        demonstrations. Consider a demonstration of adding chicken (visiting the yellow region) and then broccoli (visiting the
                        green region) to a pot (visiting the gray region). After individual DS of visiting the yellow, the green, and the gray
                        region are learned, they can be recombined given a new LTL (refer to the paper) to solve new tasks such as (1) adding
                        broccoli and then chicken, (2) adding only chicken, (3) continuously adding chicken. Note the white region represents an
                        empty spoon and crossing from yellow/green to white means spilling the food.
                    </div>
                </ul>
            </div>
        </div><br>

        <div class="row">
            <div class="col-md-12">
                <ul>
                    <table class="center">
                        <tr style=font-size:15pt>
                            <td style='text-align:center; vertical-align:middle; color:#A31F34' colspan="4">
                                Line Inspection Task
                                <p>
                            </td>
                        </tr>
                        <tr>
                            <td>
                                <video width="100%" class="center" controls>
                                    <source src="figs/exp2.mp4" type="video/mp4">
                                </video>
                            </td>
                        </tr>
                    </table>
                </ul>
            </div>
        </div><br>

        <div class="row">
            <div class="col-md-12">
                <ul>
                    <table class="center">
                        <tr style=font-size:15pt>
                            <td style='text-align:center; vertical-align:middle; color:#A31F34' colspan="4">
                                Color Tracing Task
                                <p>
                            </td>
                        </tr>
                        <tr>
                            <td>
                                <video width="100%" class="center" controls>
                                    <source src="figs/exp3.mp4" type="video/mp4">
                                </video>
                            </td>
                        </tr>
                    </table>
                </ul>
            </div>
        </div><br>

        <div class="row">
            <div class="col-md-12">
                <ul>
                    <table class="center">
                        <tr style=font-size:15pt>
                            <td style='text-align:center; vertical-align:middle; color:#A31F34' colspan="4">
                                Scooping Task
                                <p>
                            </td>
                        </tr>
                        <tr>
                            <td>
                                <video width="100%" class="center" controls>
                                    <source src="figs/exp1_human_perturb.mp4" type="video/mp4">
                                </video>
                            </td>
                        </tr>
                    </table>
                </ul>
            </div>
        </div><br> -->

    </div><br>


    <!-- Poster -->
    <div class="container">
        <h4 id="Related" style="padding-top: 30px; margin-top: -40px;">Related Works - Prior work that GLiDE extends by learning instead of engineering sensor models</h4>
        <hr>


        <table class="center">
        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                    <img src='figs/robot_1.jpg' width="100%">
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://yanweiw.github.io/tli/">
                    <papertitle>Temporal Logic Imitation: Learning Plan-Satisficing Motion Policies from Demonstrations
                    </papertitle>
                </a>
                <br>
                <strong>Yanwei Wang</strong>,
                Nadia Figueroa, Shen Li, Ankit Shah, Julie Shah
                <em><br>
                    <a href="https://arxiv.org/abs/2206.04632">arxiv</a>
                    /
                    <a href="https://github.com/yanweiw/tli">code</a>
                    /
                    <a href="https://yanweiw.github.io/tli/">project page</a><br>
                    <strong>CoRL 2022</strong> (<strong style="color:red;">Oral</strong>, acceptance rate: 6.5%) <br>
                    <strong>IROS 2023 Workshop</strong> (<strong style="color:red;"> Best Student Paper</strong>, Learning Meets
                    Model-based Methods for Manipulation and Grasping Workshop)
                </em><br>
        
                <!-- / -->
                <!-- <a href="https://arxiv.org/abs/2106.01970">arXiv</a> -->
                <!-- / -->
                <!-- <a href="https://www.youtube.com/watch?v=UUVSPJlwhPg">video</a> -->
                <!-- <p></p> -->
                <p>We present a continuous motion imitation method that can provably satisfy any discrete plan specified by a
                    Linear Temporal Logic (LTL) formula. Consequently, the imitator is robust to both task- and motion-level
                    disturbances and guaranteed to achieve task success.</p>
            </td>
        </table>

        


    </div><br><br>

    <!-- Bootstrap core JavaScript -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.11.0/umd/popper.min.js" integrity="sha384-b/U6ypiBEHpOf/4+1nzFpr53nxSS+GLCkfwBdFNTxtclqqenISfwAzpKaMNFNmj4" crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/js/bootstrap.min.js" integrity="sha384-h0AbiXch4ZDo7tp9hKZ4TsHbi047NrKGLO3SEJAg45jXxnGIfYzk4Si90RDIqNm1" crossorigin="anonymous"></script>

</body>

</html>